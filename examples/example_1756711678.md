**1. Auctions Meet Bandits: An Empirical Analysis**  
[Link](https://arxiv.org/format/2508.21162)  
This paper empirically studies the interplay between sponsored search auctions and multi‑armed bandits, showing that the standard Thompson‑Sampling strategy can be sub‑optimal in this setting. By tailoring exploration to keyword characteristics, the authors achieve a significant revenue‑efficiency trade‑off, providing actionable policies for ad platforms.

**2. Adaptive LLM Routing under Budget Constraints**  
[Link](https://arxiv.org/format/2508.21141)  
The authors cast LLM routing as a contextual bandit problem, learning a shared embedding space for queries and models. Their PILOT algorithm adaptively selects the most suitable LLM while respecting a user‑defined budget, outperforming supervised routing baselines.

**3. Model‑Task Alignment Drives Distinct RL Outcomes**  
[Link](https://arxiv.org/format/2508.21188)  
This survey identifies model‑task alignment—measured by pass@k accuracy—as the key factor behind many counter‑intuitive RL phenomena. Systematic experiments across architectures and tasks reveal when standard RL succeeds versus when alignment‑driven tricks are needed.

**4. PVPO: Pre‑Estimated Value‑Based Policy Optimization for Agentic Reasoning**  
[Link](https://arxiv.org/format/2508.21104)  
PVPO introduces a critic‑free RL method that uses an advantage reference anchor and data pre‑sampling to reduce bias. On nine benchmark domains it attains state‑of‑the‑art performance while requiring fewer rollouts than traditional policy‑gradient methods.

**5. Beyond Prediction: Reinforcement Learning as the Defining Leap in Healthcare AI**  
[Link](https://arxiv.org/format/2508.21101)  
This comprehensive survey frames RL as a shift from predictive modeling to decision‑making in healthcare. It covers model‑based, offline, and reward‑design techniques, and discusses ethical and deployment challenges unique to clinical settings.

**6. R‑4B: Incentivizing General‑Purpose Auto‑Thinking Capability in MLLMs via Bi‑Mode Annealing and Reinforce Learning**  
[Link](https://arxiv.org/format/2508.21113)  
R‑4B equips a multimodal LLM with an adaptive “think‑or‑not” switch, trained via bi‑mode annealing and BPO. The resulting model matches or surpasses larger baselines on 25 reasoning benchmarks while saving computational cost.

**7. Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?**  
[Link](https://arxiv.org/format/2508.21087)  
The study proposes personality prompting with LLMs to synthesize verbal and non‑verbal agent behaviors. Experiments in negotiation and ice‑breaking scenarios show that generated behaviors align with intended traits and are perceptible to human observers.

**8. Can Multimodal LLMs Solve the Basic Perception Problems of Percept‑V?**  
[Link](https://arxiv.org/format/2508.21143)  
A new dataset, Percept‑V, tests basic visual perception skills on synthetic images. The paper evaluates leading MLLMs and finds a sharp decline in accuracy with increasing task complexity, highlighting current limitations in perception reasoning.

**9. The Hidden Cost of Defaults in Recommender System Evaluation**  
[Link](https://arxiv.org/format/2508.21180)  
Auditing the RecBole framework reveals an undocumented early‑stopping policy that biases hyper‑parameter searches. The authors quantify the variance introduced by this hidden default and recommend more transparent tuning practices.

**10. Model‑Driven Quantum Code Generation Using Large Language Models and Retrieval‑Augmented Generation**  
[Link](https://arxiv.org/format/2508.21197)  
This paper explores a new direction for model‑to‑text/code transformation, leveraging LLMs enhanced with RAG pipelines to generate quantum code. Experiments on Qiskit code snippets show up to a four‑fold improvement in CodeBLEU scores, demonstrating the promise of retrieval‑augmented generation for domain‑specific code synthesis.