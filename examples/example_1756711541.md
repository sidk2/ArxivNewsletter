1. **2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in Autonomous Driving**  
   [https://arxiv.org/format/2508.21080](https://arxiv.org/format/2508.21080)  
   This workshop gathers researchers and industry experts to tackle novel, out‑of‑distribution hazards in autonomous driving. It focuses on anomaly detection, open‑set recognition, and vision‑language models for hazard understanding, aiming to push the state of the art in safe autonomous driving.

2. **Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment**  
   [https://arxiv.org/format/2508.21090](https://arxiv.org/format/2508.21090)  
   Q‑Align introduces Query‑Query alignment to mitigate attention leakage in zero‑shot appearance transfer, improving semantic alignment between source and target images. Experiments show superior appearance fidelity while preserving structural consistency compared to existing methods.

3. **Video-LLMs with Temporal Visual Screening**  
   [https://arxiv.org/format/2508.21094](https://arxiv.org/format/2508.21094)  
   The paper proposes Temporal Visual Screening (TVS), a front‑end adapter that focuses on salient video segments during training and inference. TVS yields significant gains in video‑language understanding, outperforming prior approaches on video trimming and query rewriting tasks.

4. **ScanMove: Motion Prediction and Transfer for Unregistered Body Meshes**  
   [https://arxiv.org/format/2508.21095](https://arxiv.org/format/2508.21095)  
   ScanMove presents a rig‑free, data‑driven framework that predicts and transfers motion on raw 3D meshes using a motion embedding network and per‑vertex feature field. Quantitative and qualitative results on walking and running tasks demonstrate its effectiveness on challenging unregistered meshes.

5. **ROBUST-MIPS: A Combined Skeletal Pose and Instance Segmentation Dataset for Laparoscopic Surgical Instruments**  
   [https://arxiv.org/format/2508.21096](https://arxiv.org/format/2508.21096)  
   This work introduces ROBUST‑MIPS, a dataset that pairs skeletal pose annotations with instance segmentation for surgical tools. It enables joint study of pose estimation and segmentation, providing a benchmark for evaluating methods in medical‑vision applications.

6. **GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions**  
   [https://arxiv.org/format/2508.21102](https://arxiv.org/format/2508.21102)  
   GENNAV predicts target existence and generates segmentation masks for stuff‑type regions in natural language instructions. Evaluated on the GRiN‑Drive benchmark and real‑world autonomous driving scenarios, it outperforms baseline methods in both metrics.

7. **EmbodiedOneVision: Interleaved Vision‑Text‑Action Pretraining for General Robot Control**  
   [https://arxiv.org/format/2508.21112](https://arxiv.org/format/2508.21112)  
   The paper introduces EO‑1, a unified embodied foundation model trained on EO‑Data1.5M, which processes images, text, video, and action. EO‑1 demonstrates strong multimodal embodied reasoning and general robot control across diverse embodiments.

8. **HiddenObject: Modality‑Agnostic Fusion for Multimodal Hidden Object Detection**  
   [https://arxiv.org/format/2508.21135](https://arxiv.org/format/2508.21135)  
   HiddenObject fuses RGB, thermal, and depth data using a Mamba‑based fusion mechanism to detect hidden or partially concealed objects. The framework achieves state‑of‑the‑art performance across multiple benchmarks, outperforming unimodal and naive fusion strategies.

9. **RadGS‑Reg: Registering Spine CT with Biplanar X-rays via Joint 3D Radiative Gaussians Reconstruction and 3D/3D Registration**  
   [https://arxiv.org/format/2508.21154](https://arxiv.org/format/2508.21154)  
   RadGS‑Reg introduces a joint 3D Radiative Gaussians reconstruction and 3D/3D registration pipeline for CT/X‑ray alignment. It surpasses existing methods on in‑house datasets, achieving state‑of‑the‑art accuracy for vertebral‑level registration.

10. **Observer Design for Optical Flow‑Based Visual‑Inertial Odometry with Almost‑Global Convergence**  
    [https://arxiv.org/format/2508.21163](https://arxiv.org/format/2508.21163)  
    This work presents a cascaded observer that fuses optical flow and IMU data to estimate velocity and gravity direction, achieving almost‑global asymptotic stability. The approach is validated through simulations, demonstrating robust visual‑inertial odometry for monocular cameras.